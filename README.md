# Deep-Learning-FNN-from-scratch
# Batch Size Optimization in Neural Networks

This project explores the impact of different batch sizes, batch normalization, and L2 regularization on training a neural network from scratch â€” without using deep learning libraries like PyTorch or TensorFlow.

## ğŸ§  Custom Neural Network Implementation

The core of this project is a fully connected feedforward neural network (FNN) **implemented from scratch in Python**, including:

- ğŸ”§ Manual forward & backward propagation
- ğŸ§® Batch Normalization implementation
- ğŸ”¥ ReLU activation for hidden layers
- ğŸ“Š Softmax activation for output layer (multi-class classification)
- ğŸ¯ Cross-entropy loss for training

All features are coded without external deep learning frameworks to fully understand the mechanics of training deep networks.

---

## ğŸ§ª Experiment Summary

### âœ… Without BatchNorm (lambda=0)
- **Best Batch Size**: `16`
- **Accuracy**: `0.9212`
- **Runtime**: `37.5s`
- âš–ï¸ *If runtime is important*: Use batch size `32` with similar accuracy (~0.88) and faster runtime.

### âœ… With BatchNorm (lambda=0)
- **Best Batch Size**: `128`
- **Accuracy**: `0.913`
- **Runtime**: `69.2s`

### âœ… With L2 Regularization (lambda=0.05), No BatchNorm
- **Best Batch Size**: `16`
- **Accuracy**: `~0.94`
- **Runtime**: `113s`

### âœ… With L2 + BatchNorm
- **Best Batch Size**: `64`
- **Accuracy**: `0.915`
- **Runtime**: `112s`

---

## ğŸ“‰ Insights

- Batch size 16 consistently gave high accuracy, but with a trade-off in runtime.
- Batch normalization shifts the optimal batch size due to added computation.
- L2 regularization had limited effect due to lack of overfitting in the base network.
- Weight differences with/without L2 were minimal.
